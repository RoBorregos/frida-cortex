"""Execution Performance Tester for Command Interpreter

This module takes JSON results from model_performance_tester.py, executes the actual commands
generated by the models, and compares the execution results with dataset_execution.json
using embeddings-based comparison for result similarity.
"""

import json
import time
from typing import Optional, List, Dict, Any
from collections import defaultdict
import sys
import os
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from scipy.spatial.distance import cosine
from tqdm import tqdm
import numpy as np
from pydantic import BaseModel
import argparse


load_dotenv()

# Add the command_interpreter directory to Python path to enable imports
# This allows us to import both baml_client and command_interpreter modules
script_dir = os.path.dirname(os.path.abspath(__file__))
benchmarks_dir = script_dir  # /path/to/frida-cortex/command_interpreter/benchmarks
command_interpreter_dir = os.path.dirname(benchmarks_dir)  # /path/to/frida-cortex/command_interpreter
root_dir = os.path.dirname(command_interpreter_dir)  # /path/to/frida-cortex

# Add both root (for command_interpreter package) and command_interpreter dir (for baml_client)
sys.path.extend([root_dir, command_interpreter_dir])

from baml_client.types import (
    CommandListLLM, GoTo, PickObject, FindPersonByName, FindPerson, Count,
    GetPersonInfo, GetVisualInfo, AnswerQuestion, FollowPersonUntil, GuidePersonTo,
    GiveObject, PlaceObject, SayWithContext
)
from baml_client.config import set_log_level
set_log_level("ERROR")
from command_interpreter.caller import execute_function, clear_command_history
from command_interpreter.tasks import Tasks
from termcolor import colored

# --- Configuration ---
STARTING_CASE = 0  # Starting test case index (0 = start from beginning)
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # Same as model_performance_tester
SIMILARITY_THRESHOLD = 0.75  # Threshold for result similarity
EXECUTION_DATASET_FILE = "dataset_generator/dataset_execution.json"

# --- Pydantic Models for Structured Results ---

class ExecutionComparison(BaseModel):
    """Individual command execution comparison"""
    command_index: int
    expected_action: str
    actual_action: str
    expected_success: bool
    actual_success: bool
    expected_result: str
    actual_result: str
    action_match: bool
    success_match: bool
    result_similarity: float
    overall_score: float


class ExecutionTestResult(BaseModel):
    """Individual test case execution result"""
    test_index: int
    input_command: str
    model_generated_commands: List[Dict]
    expected_execution_results: List[Dict]
    actual_execution_results: List[Dict]
    command_comparisons: List[ExecutionComparison]
    overall_execution_score: float
    passed: bool
    execution_time: float
    error: Optional[str] = None
    cmd_category: Optional[str] = None


class ExecutionCommandCountGroup(BaseModel):
    """Results grouped by command count"""
    command_count: int
    passed_count: int
    failed_count: int
    total_count: int
    pass_rate: float
    average_execution_score: float
    test_results: List[ExecutionTestResult]


class ExecutionTaskTypeGroup(BaseModel):
    """Results grouped by task type"""
    task_type: str
    task_description: str
    passed_count: int
    failed_count: int
    total_count: int
    pass_rate: float
    average_execution_score: float
    test_results: List[ExecutionTestResult]


class ExecutionTestSummary(BaseModel):
    """Complete execution test summary"""
    model_name: str
    grounding_enabled: bool
    total_cases: int
    total_passed: int
    total_failed: int
    overall_pass_rate: float
    average_execution_score: float
    average_execution_time: float
    groups_by_command_count: List[ExecutionCommandCountGroup]
    groups_by_task_type: List[ExecutionTaskTypeGroup]
    
    def print_summary(self):
        """Print a formatted summary of the execution test results"""
        print("\n" + "=" * 80)
        print(f"EXECUTION TEST SUMMARY - Model: {self.model_name}")
        print(f"Grounding: {'ENABLED' if self.grounding_enabled else 'DISABLED'}")
        print("=" * 80)
        print(f"Total Cases: {self.total_cases}")
        print(f"Passed: {self.total_passed} ({self.overall_pass_rate:.1f}%)")
        print(f"Failed: {self.total_failed}")
        print(f"Average Execution Score: {self.average_execution_score:.3f}")
        print(f"Average Execution Time: {self.average_execution_time:.2f}s")
        
        print(f"\nResults by Command Count:")
        print("-" * 60)
        for group in sorted(self.groups_by_command_count, key=lambda x: x.command_count):
            print(f"Commands: {group.command_count:2d} | "
                  f"Total: {group.total_count:3d} | "
                  f"Passed: {group.passed_count:3d} | "
                  f"Failed: {group.failed_count:3d} | "
                  f"Pass Rate: {group.pass_rate:5.1f}% | "
                  f"Avg Score: {group.average_execution_score:.3f}")
        
        print(f"\nResults by Task Type:")
        print("-" * 80)
        for group in sorted(self.groups_by_task_type, key=lambda x: x.task_type):
            print(f"Task {group.task_type}: {group.task_description}")
            print(f"  Total: {group.total_count:3d} | "
                  f"Passed: {group.passed_count:3d} | "
                  f"Failed: {group.failed_count:3d} | "
                  f"Pass Rate: {group.pass_rate:5.1f}% | "
                  f"Avg Score: {group.average_execution_score:.3f}")
        print("=" * 80)


# --- Helper Functions ---

# Load the embedding model globally
print(f"Loading embedding model: {EMBEDDING_MODEL}...")
try:
    embedding_model = SentenceTransformer(EMBEDDING_MODEL)
    print("Embedding model loaded.")
except Exception as e:
    print(f"\x1b[91mError loading sentence transformer model '{EMBEDDING_MODEL}': {e}\x1b[0m")
    print("\x1b[93mPlease ensure 'sentence-transformers' and 'torch' are installed.\x1b[0m")
    print("Result similarity will default to exact string matching.")
    embedding_model = None


def calculate_result_similarity(result1: str, result2: str) -> float:
    """Calculate similarity between two execution results using embeddings"""
    if not result1 and not result2:
        return 1.0
    if not result1 or not result2:
        return 0.0
    
    if embedding_model:
        try:
            embeddings = embedding_model.encode([result1, result2])
            if embeddings.shape[0] != 2:
                return 1.0 if result1 == result2 else 0.0
            
            similarity = 1 - cosine(embeddings[0], embeddings[1])
            return float(np.nan_to_num(similarity))
        except Exception as e:
            print(f"Error calculating similarity for '{result1}' vs '{result2}': {e}")
            return 1.0 if result1 == result2 else 0.0
    else:
        return 1.0 if result1 == result2 else 0.0


def reconstruct_command_object(command_data: Dict):
    """Reconstruct a command object from JSON data"""
    try:
        action = command_data.get("action")
        
        # Map action to appropriate command class
        # This mapping should match the AvailableCommands union type
        if action == "go_to":
            return GoTo(action=action, location_to_go=command_data.get("location_to_go", ""))
        elif action == "pick_object":
            return PickObject(action=action, object_to_pick=command_data.get("object_to_pick", ""))
        elif action == "find_person_by_name":
            return FindPersonByName(action=action, name=command_data.get("name", ""))
        elif action == "find_person":
            return FindPerson(action=action, attribute_value=command_data.get("attribute_value", ""))
        elif action == "count":
            return Count(action=action, target_to_count=command_data.get("target_to_count", ""))
        elif action == "get_person_info":
            return GetPersonInfo(action=action, info_type=command_data.get("info_type", ""))
        elif action == "get_visual_info":
            return GetVisualInfo(action=action, 
                               measure=command_data.get("measure", ""),
                               object_category=command_data.get("object_category", ""))
        elif action == "answer_question":
            return AnswerQuestion(action=action)
        elif action == "follow_person_until":
            return FollowPersonUntil(action=action, destination=command_data.get("destination", ""))
        elif action == "guide_person_to":
            return GuidePersonTo(action=action, destination_room=command_data.get("destination_room", ""))
        elif action == "give_object":
            return GiveObject(action=action)
        elif action == "place_object":
            return PlaceObject(action=action)
        elif action == "say_with_context":
            return SayWithContext(action=action,
                                user_instruction=command_data.get("user_instruction", ""),
                                previous_command_info=command_data.get("previous_command_info", ""))
        else:
            raise ValueError(f"Unknown action: {action}")
            
    except Exception as e:
        print(f"Error reconstructing command from {command_data}: {e}")
        raise


def execute_commands_and_capture_results(command_list: List[Dict], tasks: Tasks, grounding: bool = True) -> List[Dict]:
    """Execute a list of commands and capture the results"""
    execution_results = []
    
    for i, command_data in enumerate(command_list):
        try:
            # Reconstruct the command object
            command_obj = reconstruct_command_object(command_data)
            
            # Execute the command and capture results
            # We need to capture the execution result manually since execute_function
            # doesn't return the result directly
            
            # Execute the command
            start_time = time.time()
            action, success, result = execute_function(command_obj, tasks, grounding)
            execution_time = time.time() - start_time
            
            # Store the execution result
            execution_results.append({
                "action": action,
                "success": success,
                "result": result
            })
            
        except Exception as e:
            print(f"Error executing command {i}: {e}")
            execution_results.append({
                "action": command_data.get("action", "unknown"),
                "success": False,
                "result": f"execution error: {str(e)}"
            })
    
    return execution_results


def compare_execution_results(expected: List[Dict], actual: List[Dict]) -> List[ExecutionComparison]:
    """Compare expected vs actual execution results"""
    comparisons = []
    
    # Ensure both lists have the same length
    min_len = min(len(expected), len(actual))
    
    for i in range(min_len):
        exp = expected[i]
        act = actual[i]
        
        action_match = exp.get("action") == act.get("action")
        success_match = exp.get("success") == act.get("success")
        
        # Only use similarity scoring for say_with_context actions, exact match for others
        if exp.get("action") == "say_with_context":
            result_similarity = calculate_result_similarity(
                str(exp.get("result", "")),
                str(act.get("result", ""))
            )
        else:
            # Exact string match for all other actions
            result_similarity = 1.0 if str(exp.get("result", "")) == str(act.get("result", "")) else 0.0
        
        # Calculate overall score for this command
        # If success status doesn't match, score is 0 regardless of other factors
        if not success_match or not action_match:
            overall_score = 0.0
        else:
            overall_score = result_similarity
        
        comparison = ExecutionComparison(
            command_index=i,
            expected_action=str(exp.get("action", "")),
            actual_action=str(act.get("action", "")),
            expected_success=bool(exp.get("success", False)),
            actual_success=bool(act.get("success", False)),
            expected_result=str(exp.get("result", "")),
            actual_result=str(act.get("result", "")),
            action_match=action_match,
            success_match=success_match,
            result_similarity=result_similarity,
            overall_score=overall_score
        )
        
        comparisons.append(comparison)
    
    return comparisons


def load_execution_dataset(file_path: str) -> List[Dict]:
    """Load the execution dataset"""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: Execution dataset file not found at {file_path}")
        return []
    except json.JSONDecodeError as e:
        print(f"Error: Could not decode JSON from {file_path}: {e}")
        return []


def select_grounding_option():
    """Interactive grounding selection"""
    while True:
        try:
            choice = input("Enable grounding for execution? (Y/n): ").strip().lower()
            if choice in ['', 'y', 'yes']:
                return True
            elif choice in ['n', 'no']:
                return False
            else:
                print("Please enter 'y' for yes or 'n' for no.")
        except KeyboardInterrupt:
            print("\nExiting...")
            return None
        except Exception as e:
            print(f"Error: {e}")


def run_execution_tests(results_json_path: str) -> ExecutionTestSummary:
    """Main function to run execution tests"""
    
    # Get grounding preference from user
    grounding_enabled = select_grounding_option()
    if grounding_enabled is None:
        return None
    
    print(f"Grounding: {'ENABLED' if grounding_enabled else 'DISABLED'}")
    
    # Load the model test results
    try:
        with open(results_json_path, 'r') as f:
            model_results = json.load(f)
    except FileNotFoundError:
        print(f"Error: Results file not found at {results_json_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"Error: Could not decode JSON from {results_json_path}: {e}")
        return None
    
    # Load the execution dataset
    execution_dataset = load_execution_dataset(EXECUTION_DATASET_FILE)
    if not execution_dataset:
        print("Error: Could not load execution dataset")
        return None
    
    model_name = model_results.get("model_name", "Unknown")
    print(f"Running execution tests for model: {model_name}")
    
    # Create a Tasks instance for execution
    tasks = Tasks()
    
    all_test_results = []
    execution_times = []
    
    # Extract test results from all groups
    all_model_test_results = []
    for group in model_results.get("groups_by_command_count", []):
        all_model_test_results.extend(group.get("test_results", []))
    
    # Filter test results based on STARTING_CASE
    if STARTING_CASE > 0:
        filtered_results = [result for result in all_model_test_results if result.get("index", -1) >= STARTING_CASE]
        print(f"Filtered {len(all_model_test_results)} test cases to {len(filtered_results)} (starting from index {STARTING_CASE})")
        all_model_test_results = filtered_results
    
    print(f"Processing {len(all_model_test_results)} test cases...")
    
    for test_result in tqdm(all_model_test_results, desc="Executing commands"):
        time.sleep(2)
        try:
            test_index = test_result.get("index", -1)
            input_command = test_result.get("input_command", "")
            actual_commands = test_result.get("actual_commands", [])
            cmd_category = test_result.get("cmd_category", "Unknown")
            
            print(f"\n--- Execution Test {test_index} ---")
            print(f"Input: {input_command}")
            
            # Find the corresponding expected execution results
            if test_index < len(execution_dataset):
                expected_execution = execution_dataset[test_index].get("structured_cmd", [])
            else:
                print(f"Warning: No execution data found for index {test_index}")
                expected_execution = []
            
            if not actual_commands:
                print("Warning: No actual commands to execute")
                continue
            
            # Execute the commands
            start_time = time.time()
            actual_execution_results = execute_commands_and_capture_results(actual_commands, tasks, grounding_enabled)
            execution_time = time.time() - start_time
            execution_times.append(execution_time)
            
            # Clear command history for next test
            clear_command_history(tasks)
            
            # Compare results
            command_comparisons = compare_execution_results(expected_execution, actual_execution_results)
            
            # Calculate overall execution score
            if command_comparisons:
                overall_execution_score = np.mean([comp.overall_score for comp in command_comparisons])
            else:
                overall_execution_score = 0.0
            
            passed = overall_execution_score >= SIMILARITY_THRESHOLD
            
            print(f"Execution Score: {overall_execution_score:.3f}")
            if passed:
                print(f" \x1b[92mPassed (Score: {overall_execution_score:.3f} >= {SIMILARITY_THRESHOLD})\x1b[0m")
            else:
                print(f" \x1b[91mFailed (Score: {overall_execution_score:.3f} < {SIMILARITY_THRESHOLD})\x1b[0m")
            
            # Create execution test result
            execution_test_result = ExecutionTestResult(
                test_index=test_index,
                input_command=input_command,
                model_generated_commands=actual_commands,
                expected_execution_results=expected_execution,
                actual_execution_results=actual_execution_results,
                command_comparisons=command_comparisons,
                overall_execution_score=overall_execution_score,
                passed=passed,
                execution_time=execution_time,
                cmd_category=cmd_category
            )
            
            all_test_results.append(execution_test_result)
            
        except Exception as e:
            print(f" \x1b[91mFailed (Error during execution): {e}\x1b[0m")
            execution_test_result = ExecutionTestResult(
                test_index=test_index,
                input_command=input_command,
                model_generated_commands=actual_commands,
                expected_execution_results=[],
                actual_execution_results=[],
                command_comparisons=[],
                overall_execution_score=0.0,
                passed=False,
                execution_time=0.0,
                error=str(e),
                cmd_category=cmd_category
            )
            all_test_results.append(execution_test_result)
    
    # Group results by command count
    groups_by_count = defaultdict(list)
    for result in all_test_results:
        command_count = len(result.model_generated_commands)
        groups_by_count[command_count].append(result)
    
    command_count_groups = []
    for command_count, results in groups_by_count.items():
        passed_count = sum(1 for r in results if r.passed)
        failed_count = len(results) - passed_count
        pass_rate = (passed_count / len(results)) * 100 if results else 0
        avg_score = np.mean([r.overall_execution_score for r in results]) if results else 0
        
        group = ExecutionCommandCountGroup(
            command_count=command_count,
            passed_count=passed_count,
            failed_count=failed_count,
            total_count=len(results),
            pass_rate=pass_rate,
            average_execution_score=avg_score,
            test_results=results
        )
        command_count_groups.append(group)
    
    # Group results by task type (using same logic as model_performance_tester)
    task_type_descriptions = {
        'A': 'Navigate to a location, look for a person, and follow',
        'B': 'Take an object from a placement, and perform an action',
        'C': 'Speak or answer a question'
    }
    
    task_type_results = defaultdict(list)
    for result in all_test_results:
        cmd_category = result.cmd_category
        if cmd_category and cmd_category != "Unknown":
            task_types = cmd_category.split('|')
            for task_type in task_types:
                task_type = task_type.strip()
                if task_type in task_type_descriptions:
                    task_type_results[task_type].append(result)
    
    task_type_groups = []
    for task_type, results in task_type_results.items():
        passed_count = sum(1 for r in results if r.passed)
        failed_count = len(results) - passed_count
        pass_rate = (passed_count / len(results)) * 100 if results else 0
        avg_score = np.mean([r.overall_execution_score for r in results]) if results else 0
        
        group = ExecutionTaskTypeGroup(
            task_type=task_type,
            task_description=task_type_descriptions[task_type],
            passed_count=passed_count,
            failed_count=failed_count,
            total_count=len(results),
            pass_rate=pass_rate,
            average_execution_score=avg_score,
            test_results=results
        )
        task_type_groups.append(group)
    
    # Calculate overall statistics
    total_passed = sum(1 for r in all_test_results if r.passed)
    total_failed = len(all_test_results) - total_passed
    overall_pass_rate = (total_passed / len(all_test_results)) * 100 if all_test_results else 0
    average_execution_score = np.mean([r.overall_execution_score for r in all_test_results]) if all_test_results else 0
    average_execution_time = np.mean(execution_times) if execution_times else 0
    
    # Create summary
    summary = ExecutionTestSummary(
        model_name=model_name,
        grounding_enabled=grounding_enabled,
        total_cases=len(all_test_results),
        total_passed=total_passed,
        total_failed=total_failed,
        overall_pass_rate=overall_pass_rate,
        average_execution_score=average_execution_score,
        average_execution_time=average_execution_time,
        groups_by_command_count=command_count_groups,
        groups_by_task_type=task_type_groups
    )
    
    # Print summary
    summary.print_summary()
    
    return summary


def main():
    """Main function with command line interface"""
    parser = argparse.ArgumentParser(
        description="Execute commands from model test results and compare with expected execution dataset"
    )
    parser.add_argument(
        "results_file",
        help="Path to the model test results JSON file"
    )
    parser.add_argument(
        "--output",
        help="Output file path for execution test results (optional)"
    )
    
    args = parser.parse_args()
    
    if not os.path.exists(args.results_file):
        print(f"Error: Results file '{args.results_file}' not found")
        return
    
    print(f"Starting execution tests for: {args.results_file}")
    
    results = run_execution_tests(args.results_file)
    
    if results:
        print(f"\nExecution test completed successfully!")
        
        if args.output:
            # Save results to JSON file
            # Add grounding info to custom filename if not already present
            output_path = args.output
            if not ("_grounded" in output_path or "_no_grounding" in output_path):
                grounding_suffix = "_grounded" if results.grounding_enabled else "_no_grounding"
                name, ext = output_path.rsplit('.', 1) if '.' in output_path else (output_path, 'json')
                output_path = f"{name}{grounding_suffix}.{ext}"
            
            with open(output_path, 'w') as f:
                json.dump(results.model_dump(), f, indent=2)
            print(f"Results saved to: {output_path}")
        else:
            # Save with auto-generated name
            model_name = results.model_name.replace(" ", "_")
            grounding_suffix = "_grounded" if results.grounding_enabled else "_no_grounding"
            timestamp = int(time.time())
            output_file = f"execution_results_{model_name}{grounding_suffix}_{timestamp}.json"
            with open(output_file, 'w') as f:
                json.dump(results.model_dump(), f, indent=2)
            print(f"Results saved to: {output_file}")
    else:
        print("Execution test failed.")


if __name__ == "__main__":
    main() 