services:
  ollama:
    container_name: frida-cortex
    build:
      context: ./inference
      dockerfile: Dockerfile
    image: frida-cortex:latest
    runtime: nvidia
    network_mode: host
    volumes:
      - ./inference:/ollama
    ports:
      - 11434:11434
    environment:
      - OLLAMA_MODELS=/ollama
    stdin_open: true
    tty: true
    entrypoint:
      ["/bin/bash", "-c", "/ollama/entrypoint.sh && tail -f /dev/null"]
